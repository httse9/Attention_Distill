Removing miniconda version 22.11.1-1
Loading miniconda version 22.11.1-1
Using dataset(s): bookcorpus 
Dataset({
    features: ['text'],
    num_rows: 20000
})
vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 7.06MB/s]
config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]config.json: 100%|██████████| 383/383 [00:00<00:00, 22.3kB/s]
Map (num_proc=4):   0%|          | 0/20000 [00:00<?, ? examples/s]Map (num_proc=4):   5%|▌         | 1000/20000 [00:00<00:07, 2381.14 examples/s]Map (num_proc=4):  25%|██▌       | 5000/20000 [00:00<00:01, 10573.98 examples/s]Map (num_proc=4):  45%|████▌     | 9000/20000 [00:00<00:00, 15496.29 examples/s]Map (num_proc=4):  60%|██████    | 12000/20000 [00:00<00:00, 14036.20 examples/s]Map (num_proc=4):  75%|███████▌  | 15000/20000 [00:01<00:00, 15403.30 examples/s]Map (num_proc=4):  90%|█████████ | 18000/20000 [00:01<00:00, 16227.34 examples/s]Map (num_proc=4): 100%|██████████| 20000/20000 [00:01<00:00, 13500.00 examples/s]
Preprocessed dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'overflow_to_sample_mapping'],
    num_rows: 40661
})
Process dataset time: 5.41895604133606 seconds.
pytorch_model.bin:   0%|          | 0.00/38.8M [00:00<?, ?B/s]pytorch_model.bin:  27%|██▋       | 10.5M/38.8M [00:00<00:00, 51.9MB/s]pytorch_model.bin:  54%|█████▍    | 21.0M/38.8M [00:00<00:00, 37.8MB/s]pytorch_model.bin:  81%|████████  | 31.5M/38.8M [00:00<00:00, 42.9MB/s]pytorch_model.bin: 100%|██████████| 38.8M/38.8M [00:01<00:00, 33.4MB/s]pytorch_model.bin: 100%|██████████| 38.8M/38.8M [00:01<00:00, 36.1MB/s]
[0.7624501019716263, 0.39673256278038027]
[0.5319522999227047, 0.2571337383240461]
[0.45128952115774157, 0.21538301073014737]
[0.3979511193931103, 0.1877918429672718]
[0.3583791509270668, 0.1675539191812277]
[0.3270123578608036, 0.15177573263645172]
[0.30131780803203584, 0.13906708732247353]
[0.27952837347984316, 0.12850073706358672]
[0.2607190772891045, 0.11953697465360165]
[0.2443536065518856, 0.11190572399646044]
[0.22980637289583683, 0.105235880240798]
[0.21687104627490045, 0.09939443729817868]
[0.20514173060655594, 0.09417854100465775]
[0.1945894356817007, 0.08954278212040663]
[0.18500745184719564, 0.08541785255074501]
[0.1762093137949705, 0.0816408209502697]
[0.1681487686932087, 0.07824113909155131]
[0.1607424605637789, 0.07515751104801893]
[0.15391379408538342, 0.07232515644282103]
[0.14755086675286294, 0.06970616783946752]
[0.14167875722050666, 0.06732335072010756]
[0.13622354865074157, 0.06510742530226707]
[0.1311013273894787, 0.06306064752861858]
[0.12634096033871173, 0.061142932437360284]
[0.12195847406983376, 0.05940149258822203]
[0.11774402912706136, 0.05772250685840845]
[0.11383848749101162, 0.056183052528649566]
[0.1102140985429287, 0.05475487625226379]
[0.10677538774907588, 0.05339500000700355]
[0.10354806277900934, 0.0521201578900218]
[0.10048416629433632, 0.05091990744695067]
[0.09761892780661582, 0.04978819666430354]
[0.09491549246013165, 0.0487324901856482]
[0.09233338478952646, 0.04770635841414332]
[0.08993716090917588, 0.04677398428320885]
[0.08765850104391575, 0.04586761761456728]
[0.08546685930341483, 0.045005824975669385]
[0.0834368746727705, 0.04419222604483366]
[0.08154087048023939, 0.04342487016692757]
[0.07966266218572855, 0.04268756061792374]
Distillation time: 70.34489011764526 seconds.
